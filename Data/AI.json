[
    {
        "Title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "Authors": [
            "Kaiming He",
            "Haoqi Fan",
            "Yuxin Wu",
            "Saining Xie",
            "Ross Girshick"
        ],
        "Abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
        "Keywords": [
            "Dictionaries",
            "Task analysis",
            "Loss measurement",
            "Unsupervised learning",
            "Buildings",
            "Visualization",
            "Training",
            "Representation Learning",
            "Momentum Contrast",
            "Unsupervised Visual Representation Learning",
            "Unsupervised Learning",
            "Self-supervised Learning",
            "Unsupervised Representation",
            "Unsupervised Representation Learning",
            "Loss Function",
            "Ablation",
            "Learning Rate",
            "Computer Vision",
            "Negative Samples",
            "Object Detection",
            "Data Augmentation",
            "Semantic Segmentation",
            "Fully-connected Layer",
            "Linear Classifier",
            "Appendix For Details",
            "Random Initialization",
            "Instance Segmentation",
            "Pretext Task",
            "Memory Bank",
            "Contrastive Loss",
            "Dictionary Size",
            "Large Momentum",
            "Accuracy Drop",
            "Encoder Network"
        ],
        "No. of Cites": "6164",
        "Views": 8783
    },
    {
        "Title": "Focal Loss for Dense Object Detection",
        "Authors": [
            "Tsung-Yi Lin",
            "Priya Goyal",
            "Ross Girshick",
            "Kaiming He",
            "Piotr Dollár"
        ],
        "Abstract": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.",
        "Keywords": [
            "Detectors",
            "Training",
            "Object detection",
            "Entropy",
            "Proposals",
            "Convolutional neural networks",
            "Feature extraction",
            "Object Detection",
            "Focal Loss",
            "Cross-entropy",
            "Cross-entropy Loss",
            "Local Setting",
            "Object Location",
            "Class Imbalance",
            "Simple Detection",
            "Two-stage Detectors",
            "One-stage Detectors",
            "Standard Cross-entropy Loss",
            "Hard Examples",
            "Loss Function",
            "Convolutional Network",
            "Positive Samples",
            "Aspect Ratio",
            "Input Image",
            "Bounding Box",
            "Two-stage Method",
            "Faster R-CNN",
            "Feature Pyramid Network",
            "Region Proposal Network",
            "Pyramid Level",
            "Easy Examples",
            "One-stage Methods",
            "One-stage Object Detection",
            "Box Regression",
            "Feature Pyramid",
            "Hinge Loss",
            "Conv Layer",
            "Computer vision",
            "object detection",
            "machine learning",
            "convolutional neural networks"
        ],
        "No. of Cites": "3802",
        "Views": 23980
    },
    {
        "Title": "Squeeze-and-Excitation Networks",
        "Authors": [
            "Jie Hu",
            "Li Shen",
            "Samuel Albanie",
            "Gang Sun",
            "Enhua Wu"
        ],
        "Abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of <inline-formula><tex-math notation=\"LaTeX\">${\\sim }$</tex-math></inline-formula>25 percent. Models and code are available at <uri>https://github.com/hujie-frank/SENet</uri>.",
        "Keywords": [
            "Computer architecture",
            "Computational modeling",
            "Convolution",
            "Task analysis",
            "Correlation",
            "Optimization",
            "Convolutional neural networks",
            "Computational Cost",
            "Convolutional Neural Network",
            "Building Blocks",
            "Feature Information",
            "Receptive Field",
            "Convolution Operation",
            "Local Receptive Field",
            "Channel-wise Feature",
            "Complex Models",
            "Deep Network",
            "Network Layer",
            "Data Augmentation",
            "Attention Mechanism",
            "Training Images",
            "Global Information",
            "Fully-connected Layer",
            "Depth Range",
            "Range Of Tasks",
            "Global Pooling",
            "Reduction Ratio",
            "Global Average Pooling",
            "Gating Mechanism",
            "Residual Unit",
            "Baseline Architecture",
            "ImageNet Dataset",
            "Pooling Operation",
            "Network Depth",
            "Small Cost",
            "Use Of Information",
            "Convolutional Layers",
            "Squeeze-and-excitation",
            "image representations",
            "attention",
            "convolutional neural networks"
        ],
        "No. of Cites": "3721",
        "Views": 25412
    },
    {
        "Title": "A ConvNet for the 2020s",
        "Authors": [
            "Zhuang Liu",
            "Hanzi Mao",
            "Chao-Yuan Wu",
            "Christoph Feichtenhofer",
            "Trevor Darrell",
            "Saining Xie"
        ],
        "Abstract": "The “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.",
        "Keywords": [
            "Computer vision",
            "Image segmentation",
            "Visualization",
            "Computational modeling",
            "Scalability",
            "Semantics",
            "Transformers",
            "Scalable",
            "Modernity",
            "Computer Vision",
            "Image Classification",
            "Object Detection",
            "Semantic Segmentation",
            "Vision Tasks",
            "Inductive Bias",
            "Vision Transformer",
            "Transformer Design",
            "Stem Cells",
            "Activation Function",
            "Convolutional Layers",
            "Fast Fourier Transform",
            "Spatial Dimensions",
            "Model Size",
            "Training Procedure",
            "Technical Training",
            "Architectural Design",
            "Depthwise Convolution",
            "Conv Layer",
            "Larger Kernel Size",
            "Transformer Block",
            "Large Kernel",
            "AdamW Optimizer",
            "Linear Layer",
            "Residual Block",
            "Downsampling Layer",
            "Design Decisions",
            "Deep learning architectures and techniques; Recognition: detection",
            "categorization",
            "retrieval; Representation learning"
        ],
        "No. of Cites": "3038",
        "Views": 6715
    },
    {
        "Title": "Masked Autoencoders Are Scalable Vision Learners",
        "Authors": [
            "Kaiming He",
            "Xinlei Chen",
            "Saining Xie",
            "Yanghao Li",
            "Piotr Dollár",
            "Ross Girshick"
        ],
        "Abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.",
        "Keywords": [
            "Training",
            "Couplings",
            "Computer vision",
            "Computational modeling",
            "Computer architecture",
            "Data models",
            "Pattern recognition",
            "Computer Vision",
            "Latent Representation",
            "Random Patches",
            "Previous Results",
            "Object Detection",
            "Data Augmentation",
            "Transfer Learning",
            "Representation Learning",
            "Semantic Segmentation",
            "Self-supervised Learning",
            "Linear Probe",
            "Linearly Separable",
            "Linear Projection",
            "Quality Of Representations",
            "Reconstruction Task",
            "Decoder Output",
            "Position Embedding",
            "Vision Transformer",
            "Big Model",
            "Denoising Autoencoder",
            "Tokenized",
            "Asymmetric Design",
            "Transformer Block",
            "Pixel Spacing",
            "Language Model",
            "Random Sampling",
            "Pretext Task",
            "Representation learning; Self-& semi-& meta- & unsupervised learning"
        ],
        "No. of Cites": "2921",
        "Views": 5923
    },
    {
        "Title": "Coordinate Attention for Efficient Mobile Network Design",
        "Authors": [
            "Qibin Hou",
            "Daquan Zhou",
            "Jiashi Feng"
        ],
        "Abstract": "Recent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call \"coordinate attention\". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention.",
        "Keywords": [
            "Image segmentation",
            "Computer vision",
            "Tensors",
            "Computational modeling",
            "Semantics",
            "Object detection",
            "Transforms",
            "Mobile Network",
            "Efficient Design",
            "Coordinate Attention",
            "Feature Maps",
            "Object Detection",
            "Attention Mechanism",
            "Position Information",
            "Spatial Orientation",
            "Precise Information",
            "Semantic Segmentation",
            "Object Of Interest",
            "Global Pooling",
            "Computational Overhead",
            "Feature Aggregation",
            "Attention Map",
            "Channel Attention",
            "Long-range Dependencies",
            "ImageNet Classification",
            "Single Feature Vector",
            "Horizontal Plane",
            "Attention Block",
            "Spatial Information",
            "Depthwise Separable Convolution",
            "Spatial Attention",
            "Global Information",
            "Neural Architecture Search",
            "PASCAL VOC",
            "Reduction Ratio",
            "Important Channel",
            "Vertical Direction"
        ],
        "No. of Cites": "2690",
        "Views": 6673
    },
    {
        "Title": "Segment Anything",
        "Authors": [
            "Alexander Kirillov",
            "Eric Mintun",
            "Nikhila Ravi",
            "Hanzi Mao",
            "Chloe Rolland",
            "Laura Gustafson",
            "Tete Xiao",
            "Spencer Whitehead",
            "Alexander C. Berg",
            "Wan-Yen Lo",
            "Piotr Dollár",
            "Ross Girshick"
        ],
        "Abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
        "Keywords": [
            "Image segmentation",
            "Computer vision",
            "Data privacy",
            "Computational modeling",
            "Data collection",
            "Data models",
            "Task analysis",
            "Computer Vision",
            "Image Segmentation",
            "Segmentation Model",
            "Segmentation Dataset",
            "Foundation Model",
            "Ambiguity",
            "Random Sampling",
            "Large Systems",
            "Semantic Segmentation",
            "Language Model",
            "Edge Detection",
            "Segmentation Task",
            "Inference Time",
            "Instance Segmentation",
            "Open Image",
            "Object Labels",
            "Automatic Evaluation",
            "Vision Transformer",
            "Image Encoder",
            "Image Embedding",
            "Interactive Segmentation",
            "Text Encoder",
            "Start Of Stage",
            "Free-form Text",
            "Real-world Use"
        ],
        "No. of Cites": "2334",
        "Views": 2938
    },
    {
        "Title": "Emerging Properties in Self-Supervised Vision Transformers",
        "Authors": [
            "Mathilde Caron",
            "Hugo Touvron",
            "Ishan Misra",
            "Hervé Jegou",
            "Julien Mairal",
            "Piotr Bojanowski",
            "Armand Joulin"
        ],
        "Abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
        "Keywords": [
            "Training",
            "Image segmentation",
            "Computer vision",
            "Semantics",
            "Layout",
            "Image retrieval",
            "Computer architecture",
            "Transformer",
            "Vision Transformer",
            "Convolutional Network",
            "Convolutional Neural Network",
            "ImageNet",
            "Semantic Segmentation",
            "Self-supervised Learning",
            "State Of The Art",
            "K-nearest Neighbor",
            "Data Augmentation",
            "Transfer Learning",
            "Intersection Over Union",
            "Multilayer Perceptron",
            "Batch Normalization",
            "Quality Characteristics",
            "Network Output",
            "Patch Size",
            "Global View",
            "Evaluation Protocol",
            "Linear Classifier",
            "Student Network",
            "Teacher Network",
            "Standard Benchmark",
            "Image Retrieval",
            "Top-1 Accuracy",
            "Contrastive Loss",
            "Pretext Task",
            "Representation learning",
            "Recognition and classification",
            "Transfer/Low-shot/Semi/Unsupervised Learning"
        ],
        "No. of Cites": "2204",
        "Views": 3081
    },
    {
        "Title": "Mask R-CNN",
        "Authors": [
            "Kaiming He",
            "Georgia Gkioxari",
            "Piotr Dollár",
            "Ross Girshick"
        ],
        "Abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.",
        "Keywords": [
            "Task analysis",
            "Semantics",
            "Feature extraction",
            "Object detection",
            "Proposals",
            "Image segmentation",
            "Quantization (signal)",
            "Object Detection",
            "Image Object",
            "Whistle",
            "Instance Segmentation",
            "Human Pose",
            "Segmentation Framework",
            "Keypoint Detection",
            "Faster Region-based Convolutional Neural Network",
            "Good Results",
            "Learning Rate",
            "Training Time",
            "Feature Maps",
            "Implementation Details",
            "Class Labels",
            "Multilayer Perceptron",
            "Class Prediction",
            "Max-pooling",
            "Semantic Segmentation",
            "Pose Estimation",
            "Fully Convolutional Network",
            "Feature Pyramid Network",
            "Region Proposal Network",
            "Human Pose Estimation",
            "COCO Dataset",
            "IoU Threshold",
            "Candidate Objects",
            "Backbone Architecture",
            "Feature Pyramid",
            "Spatial Layout",
            "Instance segmentation",
            "object detection",
            "pose estimation",
            "convolutional neural network"
        ],
        "No. of Cites": "2072",
        "Views": 20063
    },
    {
        "Title": "Exploring Simple Siamese Representation Learning",
        "Authors": [
            "Xinlei Chen",
            "Kaiming He"
        ],
        "Abstract": "Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our \"SimSiam\" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available.<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
        "Keywords": [
            "Visualization",
            "Computer vision",
            "Codes",
            "Shape",
            "Computational modeling",
            "Computer architecture",
            "Tools",
            "Representation Learning",
            "Negative Samples",
            "ImageNet",
            "Siamese Network",
            "Batch Size",
            "Hidden Layer",
            "Stochastic Gradient Descent",
            "Batch Normalization",
            "Self-supervised Learning",
            "Image X",
            "Hypersphere",
            "Constant Output",
            "Inductive Bias",
            "Clustering-based Methods",
            "kNN Classifier",
            "Memory Bank",
            "Symmetric Loss"
        ],
        "No. of Cites": "1911",
        "Views": 2648
    },
    {
        "Title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
        "Authors": [
            "Sixiao Zheng",
            "Jiachen Lu",
            "Hengshuang Zhao",
            "Xiatian Zhu",
            "Zekun Luo",
            "Yabiao Wang",
            "Yanwei Fu",
            "Jianfeng Feng",
            "Tao Xiang",
            "Philip H.S. Torr",
            "Li Zhang"
        ],
        "Abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",
        "Keywords": [
            "Image segmentation",
            "Visualization",
            "Semantics",
            "Transformers",
            "Decoding",
            "Pattern recognition",
            "Servers",
            "Semantic Segmentation",
            "Spatial Resolution",
            "State Of The Art",
            "Receptive Field",
            "Segmentation Model",
            "Attention Module",
            "Competitive Results",
            "Fully Convolutional Network",
            "Transformer Layers",
            "Competitive Test",
            "Visual Concepts",
            "Validation Set",
            "Feature Maps",
            "Feature Representation",
            "Model Design",
            "Multilayer Perceptron",
            "Training Images",
            "Spatial Attention",
            "Training Schedule",
            "Dilated Convolution",
            "Auxiliary Loss",
            "Multi-head Self-attention",
            "Quadratic Complexity",
            "Image X",
            "Stack Of Convolutional Layers",
            "Semantic Segmentation Models",
            "Random Horizontal Flipping",
            "Feature Representation Learning",
            "1D Sequence",
            "Long-range Dependencies"
        ],
        "No. of Cites": "1847",
        "Views": 3453
    },
    {
        "Title": "Federated Learning in Mobile Edge Networks: A Comprehensive Survey",
        "Authors": [
            "Wei Yang Bryan Lim",
            "Nguyen Cong Luong",
            "Dinh Thai Hoang",
            "Yutao Jiao",
            "Ying-Chang Liang",
            "Qiang Yang",
            "Dusit Niyato",
            "Chunyan Miao"
        ],
        "Abstract": "In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.",
        "Keywords": [
            "Training",
            "Servers",
            "Data privacy",
            "Data models",
            "Optimization",
            "Privacy",
            "Computational modeling",
            "Federated Learning",
            "Mobile Edge",
            "Mobile Edge Network",
            "Resource Allocation",
            "Mobile Devices",
            "Machine Learning Models",
            "Local Data",
            "Machine Learning Approaches",
            "Data Privacy",
            "Privacy Issues",
            "Mobile Network",
            "Updated Model",
            "Computational Capabilities",
            "Edge Computing",
            "Communication Cost",
            "Secure Communication",
            "Communication Resources",
            "Edge Server",
            "Mobile Edge Computing",
            "Vehicular Networks",
            "Federated Learning Algorithm",
            "Local Updates",
            "Internet Of Things Devices",
            "Global Model",
            "Deep Reinforcement Learning",
            "Base Station",
            "Local Training",
            "Computation Offloading",
            "Edge Nodes",
            "Federated Learning Model",
            "Federated learning",
            "mobile edge networks",
            "resource allocation",
            "communication cost",
            "data privacy",
            "data security"
        ],
        "No. of Cites": "1433",
        "Views": 36835
    },
    {
        "Title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
        "Authors": [
            "Syed Waqas Zamir",
            "Aditya Arora",
            "Salman Khan",
            "Munawar Hayat",
            "Fahad Shahbaz Khan",
            "Ming–Hsuan Yang"
        ],
        "Abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",
        "Keywords": [
            "Computational modeling",
            "Computer architecture",
            "Transformer cores",
            "Transformers",
            "Data models",
            "Image restoration",
            "Task analysis",
            "High-resolution Images",
            "Transformation Efficiency",
            "Spatial Resolution",
            "Convolutional Neural Network",
            "Attention Mechanism",
            "Feed-forward Network",
            "Large Image",
            "Transformer Model",
            "Restoration Tasks",
            "Input Features",
            "Additive Noise",
            "Spatial Dimensions",
            "Benchmark Datasets",
            "Representation Learning",
            "Small Patches",
            "Image Patches",
            "Large Patches",
            "Gating Mechanism",
            "Attention Map",
            "Long-range Dependencies",
            "Depthwise Convolution",
            "Progressive Learning",
            "Transformer Block",
            "Vision Transformer",
            "Multi-head Self-attention",
            "dB Improvement",
            "Linear Complexity",
            "Early Epoch",
            "dB Gain",
            "Local Context",
            "Low-level vision; Computational photography; Deep learning architectures and techniques"
        ],
        "No. of Cites": "1293",
        "Views": 3386
    },
    {
        "Title": "CvT: Introducing Convolutions to Vision Transformers",
        "Authors": [
            "Haiping Wu",
            "Bin Xiao",
            "Noel Codella",
            "Mengchen Liu",
            "Xiyang Dai",
            "Lu Yuan",
            "Lei Zhang"
        ],
        "Abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.",
        "Keywords": [
            "Convolutional codes",
            "Image resolution",
            "Image recognition",
            "Computer architecture",
            "Performance gain",
            "Transformers",
            "Distortion",
            "Vision Transformer",
            "Convolutional Vision Transformer",
            "Convolutional Neural Network",
            "Vision Tasks",
            "Fewer Parameters",
            "Floating-point Operations",
            "Top-1 Accuracy",
            "Positional Encoding",
            "Transformer Block",
            "Dynamic Attention",
            "Computational Cost",
            "Convolutional Layers",
            "Local Context",
            "Local Structure",
            "Input Image",
            "Image Classification",
            "Feature Maps",
            "Feature Dimension",
            "Convolution Operation",
            "Convolution Kernel",
            "Position Embedding",
            "Depthwise Separable Convolution",
            "Multistage Design",
            "Local Receptive Field",
            "Concurrent Work",
            "Transformer Architecture",
            "Non-overlapping Patches",
            "Multi-head Self-attention",
            "Efficiency Benefits",
            "Linear Projection",
            "Recognition and classification"
        ],
        "No. of Cites": "1264",
        "Views": 2838
    },
    {
        "Title": "Can AI Help in Screening Viral and COVID-19 Pneumonia?",
        "Authors": [
            "Muhammad E. H. Chowdhury",
            "Tawsifur Rahman",
            "Amith Khandakar",
            "Rashid Mazhar",
            "Muhammad Abdul Kadir",
            "Zaid Bin Mahbub",
            "Khandakar Reajul Islam",
            "Muhammad Salman Khan",
            "Atif Iqbal",
            "Nasser Al Emadi",
            "Mamun Bin Ibne Reaz",
            "Mohammad Tariqul Islam"
        ],
        "Abstract": "Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.",
        "Keywords": [
            "Diseases",
            "Lung",
            "Databases",
            "X-ray imaging",
            "Machine learning",
            "Tools",
            "COVID-19",
            "COVID-19 Pneumonia",
            "Deep Learning",
            "Convolutional Neural Network",
            "Deep Network",
            "Artificial Intelligence",
            "Chest X-ray",
            "Transfer Learning",
            "Deep Convolutional Neural Network",
            "COVID-19 Diagnosis",
            "Pre-trained Network",
            "Normal Images",
            "Pandemic Disease",
            "Image Augmentation",
            "Transfer Learning Technique",
            "Chest X-ray Images",
            "COVID-19 Detection",
            "Need For Preventive Measures",
            "Normal X-ray",
            "Normal Chest X-ray",
            "Shallow Network",
            "Small Datasets",
            "COVID-19 Patients",
            "Number Of Images",
            "Deep Learning Techniques",
            "Large Datasets",
            "Two-class Problem",
            "Deeper Layers",
            "Original Draft",
            "Severe Acute Respiratory Syndrome Coronavirus 2",
            "Artificial intelligence",
            "COVID-19 pneumonia",
            "machine learning",
            "transfer learning",
            "viral pneumonia",
            "computer-aided diagnostic tool"
        ],
        "No. of Cites": "1164",
        "Views": 28920
    },
    {
        "Title": "Federated Learning With Differential Privacy: Algorithms and Performance Analysis",
        "Authors": [
            "Kang Wei",
            "Jun Li",
            "Ming Ding",
            "Chuan Ma",
            "Howard H. Yang",
            "Farhad Farokhi",
            "Shi Jin",
            "Tony Q. S. Quek",
            "H. Vincent Poor"
        ],
        "Abstract": "Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients’ private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients’ side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula> of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>-client random scheduling strategy, where <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> (<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$1\\leq K&lt; N$ </tex-math></inline-formula>) clients are randomly selected from the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$N$ </tex-math></inline-formula> overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>-client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels.",
        "Keywords": [
            "Convergence",
            "Privacy",
            "Servers",
            "Training",
            "Analytical models",
            "Distributed databases",
            "Federated Learning",
            "Differential Privacy",
            "Loss Function",
            "Neural Network",
            "Deep Neural Network",
            "Data Privacy",
            "Level Of Protection",
            "Privacy Protection",
            "Information Leakage",
            "Random Strategy",
            "Model Aggregation",
            "Convergence Performance",
            "Artificial Noise",
            "Privacy Level",
            "Distributed Machine Learning",
            "Federated Learning Model",
            "Federated Learning Algorithm",
            "Theoretical Analysis",
            "Gaussian Noise",
            "Dataset Size",
            "Additive Noise",
            "Downlink Channel",
            "Stochastic Gradient Descent",
            "Local Training",
            "Noise Term",
            "Trainable Parameters",
            "Client-side",
            "Local Updates",
            "Multi-party Computation",
            "Empirical Risk Minimization",
            "Federated learning",
            "differential privacy",
            "convergence performance",
            "information leakage",
            "client selection"
        ],
        "No. of Cites": "1161",
        "Views": 40902
    },
    {
        "Title": "EnlightenGAN: Deep Light Enhancement Without Paired Supervision",
        "Authors": [
            "Yifan Jiang",
            "Xinyu Gong",
            "Ding Liu",
            "Yu Cheng",
            "Chen Fang",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Pan Zhou",
            "Zhangyang Wang"
        ],
        "Abstract": "Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">EnlightenGAN</i>, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/VITA-Group/EnlightenGAN</uri>.",
        "Keywords": [
            "Training",
            "Visualization",
            "Lighting",
            "Generative adversarial networks",
            "Gallium nitride",
            "Adaptation models",
            "Training data",
            "Light Enhancement",
            "Attention Mechanism",
            "Paired Data",
            "Image Pairs",
            "Generative Adversarial Networks",
            "Visual Quality",
            "Visual Scene",
            "Image Enhancement",
            "Real-world Images",
            "Training Pairs",
            "Perceptual Loss",
            "Lack Of Training Data",
            "Low-light Image",
            "Deep Learning",
            "Human Subjects",
            "Image Quality",
            "Denoising",
            "Validation Set",
            "Convolutional Layers",
            "Input Image",
            "Low Light",
            "Output Image",
            "Adaptive Histogram Equalization",
            "Normal Light",
            "Attention Map",
            "Domain Adaptation",
            "Normal Images",
            "Feature Maps",
            "Super-resolution",
            "Light Images",
            "Low-light enhancement",
            "generative adversarial networks",
            "unsupervised learning"
        ],
        "No. of Cites": "1117",
        "Views": 15059
    },
    {
        "Title": "Masked-attention Mask Transformer for Universal Image Segmentation",
        "Authors": [
            "Bowen Cheng",
            "Ishan Misra",
            "Alexander G. Schwing",
            "Alexander Kirillov",
            "Rohit Girdhar"
        ],
        "Abstract": "Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K).",
        "Keywords": [
            "Image segmentation",
            "Shape",
            "Computational modeling",
            "Semantics",
            "Computer architecture",
            "Transformers",
            "Feature extraction",
            "Image Segmentation",
            "Semantic Segmentation",
            "Segmentation Task",
            "Instance Segmentation",
            "Specific Architecture",
            "Masked Images",
            "Popular Datasets",
            "Image Segmentation Tasks",
            "Learning Rate",
            "Feature Maps",
            "Bounding Box",
            "Small Objects",
            "Final Loss",
            "Universal Model",
            "Feature Pyramid",
            "Standard Benchmark",
            "High-resolution Features",
            "Limited Computational Resources",
            "Semantic Segmentation Task",
            "Transformer Decoder",
            "Query Features",
            "Mask R-CNN",
            "COCO Dataset",
            "Segmentation Annotations",
            "Position Embedding",
            "Concurrent Work",
            "Architecture For Segmentation",
            "Fully Convolutional Network",
            "Feed-forward Network",
            "Segmentation",
            "grouping and shape analysis; Recognition: detection",
            "categorization",
            "retrieval"
        ],
        "No. of Cites": "1087",
        "Views": 2055
    },
    {
        "Title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications",
        "Authors": [
            "Shaoxiong Ji",
            "Shirui Pan",
            "Erik Cambria",
            "Pekka Marttinen",
            "Philip S. Yu"
        ],
        "Abstract": "Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.",
        "Keywords": [
            "Cognition",
            "Knowledge based systems",
            "Semantics",
            "Knowledge acquisition",
            "Task analysis",
            "Taxonomy",
            "Extraterrestrial measurements",
            "Scoring Function",
            "Knowledge Acquisition",
            "Human Cognition",
            "Representation Learning",
            "Representation Of Space",
            "Inference Rules",
            "Logical Reasoning",
            "Embedding Methods",
            "Auxiliary Information",
            "Promising Research Direction",
            "Encoding Model",
            "Temporal Graph",
            "Neural Network",
            "Neural Model",
            "Long Short-term Memory",
            "Vector Space",
            "Temporal Information",
            "Downstream Applications",
            "Graph Convolutional Network",
            "Graph Neural Networks",
            "Relation Extraction",
            "Relative Path",
            "Graph Attention Network",
            "Semantic Matching",
            "Bilinear Model",
            "Related Entities",
            "Entity Types",
            "Knowledge Representation",
            "Monte Carlo Tree Search",
            "Named Entity Recognition",
            "Deep learning",
            "knowledge graph completion (KGC)",
            "knowledge graph",
            "reasoning",
            "relation extraction",
            "representation learning"
        ],
        "No. of Cites": "1048",
        "Views": 40148
    },
    {
        "Title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
        "Authors": [
            "Yunjey Choi",
            "Youngjung Uh",
            "Jaejun Yoo",
            "Jung-Woo Ha"
        ],
        "Abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset are available at https://github.com/clovaai/stargan-v2.",
        "Keywords": [
            "Generators",
            "Training",
            "Visualization",
            "Transforms",
            "Image generation",
            "Animals",
            "Image reconstruction",
            "Image Synthesis",
            "Diverse Images",
            "StarGAN V2",
            "Scalable",
            "Visual Quality",
            "Translational Model",
            "Visual Domain",
            "Face Dataset",
            "Animal Faces",
            "Input Image",
            "Random Noise",
            "Generative Adversarial Networks",
            "Reference Image",
            "Source Images",
            "Target Domain",
            "Image Domain",
            "Multiple Outputs",
            "Reconstruction Loss",
            "Paired Box",
            "Fréchet Inception Distance",
            "Latent Code",
            "Variety Of Styles",
            "Image X",
            "Style Image",
            "Unseen Images",
            "StyleGAN",
            "Latent Vector",
            "Translation Method"
        ],
        "No. of Cites": "1029",
        "Views": 1872
    },
    {
        "Title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
        "Authors": [
            "Wei-Ning Hsu",
            "Benjamin Bolte",
            "Yao-Hung Hubert Tsai",
            "Kushal Lakhotia",
            "Ruslan Salakhutdinov",
            "Abdelrahman Mohamed"
        ],
        "Abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.<xref ref-type=\"fn\" rid=\"fn1\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><sup>1</sup></xref><xref ref-type=\"fn\" rid=\"fn2\" xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><sup>2</sup></xref>",
        "Keywords": [
            "Predictive models",
            "Representation learning",
            "Self-supervised learning",
            "Representation Learning",
            "Self-supervised Learning",
            "Speech Representations",
            "Speech Representation Learning",
            "Language Model",
            "Prediction Loss",
            "Masked Images",
            "Target Label",
            "Continuous Input",
            "Acoustic Model",
            "Word Error Rate",
            "Mutual Information",
            "Discrete Data",
            "Cluster Assignment",
            "Gaussian Mixture Model",
            "Cluster Model",
            "Training Iterations",
            "Iterative Refinement",
            "Clustering Quality",
            "Natural Language Processing Applications",
            "Fine-tuned Model",
            "Pseudo Labels",
            "Ensemble Clustering",
            "Different Numbers Of Clusters",
            "Training Labels",
            "Transformer Layers",
            "Convolutional Encoder",
            "Pretext Task",
            "Languages And Dialects",
            "Self-supervised learning",
            "BERT"
        ],
        "No. of Cites": "1007",
        "Views": 8868
    },
    {
        "Title": "Billion-Scale Similarity Search with GPUs",
        "Authors": [
            "Jeff Johnson",
            "Matthijs Douze",
            "Hervé Jégou"
        ],
        "Abstract": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</i>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</i>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy <i xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</i>-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
        "Keywords": [
            "Graphics processing units",
            "Quantization (signal)",
            "Big Data",
            "Indexing",
            "Task analysis",
            "Random access memory",
            "State Of The Art",
            "Exhaustive Search",
            "Distance Calculation",
            "Learning Algorithms",
            "Basic Reproduction Number",
            "Set Of Elements",
            "Selection Algorithm",
            "Lookup Table",
            "Memory Usage",
            "Binary Code",
            "Nearest Neighbor Search",
            "Multiple Passages",
            "GPU Memory",
            "Single GPU",
            "Destination Image",
            "Exact Search",
            "Memory Bandwidth",
            "Shared Memory",
            "Query Vector",
            "GPU Implementation",
            "Locality Sensitive Hashing",
            "Parallel Steps",
            "L2 Cache",
            "Compile Time",
            "Centroid",
            "Distance Matrix",
            "Large Collection",
            "Similarity search",
            "multimedia databases",
            "indexing methods",
            "graphical processing units"
        ],
        "No. of Cites": "1006",
        "Views": 8576
    },
    {
        "Title": "Designing Network Design Spaces",
        "Authors": [
            "Ilija Radosavovic",
            "Raj Prateek Kosaraju",
            "Ross Girshick",
            "Kaiming He",
            "Piotr Dollár"
        ],
        "Abstract": "In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.",
        "Keywords": [
            "Computational modeling",
            "Manuals",
            "Tools",
            "Sociology",
            "Statistics",
            "Training",
            "Visualization",
            "Network Design",
            "Design Space",
            "Rational Design",
            "Parametrized",
            "Manual Design",
            "Standard Model",
            "Design Process",
            "Training Time",
            "General Principles",
            "Search Space",
            "Individual Models",
            "Step Function",
            "Weight Decay",
            "Population Model",
            "Design Choices",
            "Mobile Network",
            "Inference Time",
            "Solid Curve",
            "Self-driving",
            "Linear Parameters",
            "Neural Architecture Search",
            "Top Model",
            "Training Setup",
            "T Model",
            "Block Type",
            "Network Depth",
            "Degrees Of Freedom",
            "Fair Comparison",
            "ImageNet"
        ],
        "No. of Cites": "965",
        "Views": 1242
    },
    {
        "Title": "Deep Learning for 3D Point Clouds: A Survey",
        "Authors": [
            "Yulan Guo",
            "Hanyun Wang",
            "Qingyong Hu",
            "Hao Liu",
            "Li Liu",
            "Mohammed Bennamoun"
        ],
        "Abstract": "Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.",
        "Keywords": [
            "Three-dimensional displays",
            "Solid modeling",
            "Deep learning",
            "Object detection",
            "Laser radar",
            "Task analysis",
            "Sensors",
            "Deep Learning",
            "Point Cloud",
            "3D Point Cloud",
            "Neural Network",
            "Object Detection",
            "3D Shape",
            "Object Tracking",
            "3D Segmentation",
            "3D Tracking",
            "Shape Classification",
            "Point Cloud Processing",
            "3D Object Detection",
            "Point Cloud Segmentation",
            "Convolutional Network",
            "Convolutional Neural Network",
            "Local Features",
            "Local Structure",
            "Feature Maps",
            "Feature Learning",
            "Global Features",
            "Semantic Segmentation",
            "3D Convolution",
            "3D Bounding Box",
            "Instance Segmentation",
            "Multilayer Perceptron",
            "Feature Points",
            "Semantic Labels",
            "Multilayer Perceptron Layer",
            "Point-based Methods",
            "Deep learning",
            "point clouds",
            "3D data",
            "shape classification",
            "shape retrieval",
            "object detection",
            "object tracking",
            "scene flow",
            "instance segmentation",
            "semantic segmentation",
            "part segmentation"
        ],
        "No. of Cites": "962",
        "Views": 37208
    },
    {
        "Title": "Multi-Stage Progressive Image Restoration",
        "Authors": [
            "Syed Waqas Zamir",
            "Aditya Arora",
            "Salman Khan",
            "Munawar Hayat",
            "Fahad Shahbaz Khan",
            "Ming-Hsuan Yang",
            "Ling Shao"
        ],
        "Abstract": "Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet.",
        "Keywords": [
            "Runtime",
            "Computational modeling",
            "Noise reduction",
            "Computer architecture",
            "Performance gain",
            "Image restoration",
            "Pattern recognition",
            "Denoising",
            "Performance Gain",
            "Spatial Details",
            "Image Deblurring",
            "Convolutional Neural Network",
            "Challenging Task",
            "Input Image",
            "Receptive Field",
            "Image Pairs",
            "Architectural Design",
            "Previous Stage",
            "Fewer Parameters",
            "Error Reduction",
            "Multi-scale Features",
            "Output Image",
            "Intermediate Features",
            "Ground Truth Image",
            "Original Resolution",
            "Image X",
            "Down-sampling Operation",
            "Smartphone Camera",
            "Original Image Resolution"
        ],
        "No. of Cites": "942",
        "Views": 2513
    }
]