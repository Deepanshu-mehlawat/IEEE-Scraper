[
    {
        "Title": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation",
        "Authors": [
            "Zongwei Zhou",
            "Md Mahfuzur Rahman Siddiquee",
            "Nima Tajbakhsh",
            "Jianming Liang"
        ],
        "Abstract": "The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects-an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.",
        "Keywords": [
            "Decoding",
            "Image segmentation",
            "Computer architecture",
            "Modeling",
            "Semantics",
            "Training",
            "Biomedical imaging",
            "Image Segmentation",
            "Skip Connections",
            "Electron Microscopy",
            "Magnetic Resonance Imaging",
            "Medical Imaging",
            "Feature Maps",
            "Semantic Segmentation",
            "Feature Fusion",
            "Network Depth",
            "Instance Segmentation",
            "Fully Convolutional Network",
            "Segmentation Dataset",
            "Mask R-CNN",
            "Medical Image Segmentation",
            "Backbone Architecture",
            "Deep Supervision",
            "Training Set",
            "Deep Neural Network",
            "Validation Set",
            "Technical Details",
            "Brain Tumor Segmentation",
            "Dense Connections",
            "Intersection Over Union",
            "Semantic Segmentation Models",
            "Inference Time",
            "Output Node",
            "Segmentation Results",
            "Decoder Network",
            "U-Net Architecture",
            "Adjacent Nodes",
            "Neuronal structure segmentation",
            "liver segmentation",
            "cell segmentation",
            "nuclei segmentation",
            "brain tumor segmentation",
            "lung nodule segmentation",
            "medical image segmentation",
            "semantic segmentation",
            "instance segmentation",
            "deep supervision",
            "model pruning",
            "Image Processing, Computer-Assisted",
            "Magnetic Resonance Imaging",
            "Neural Networks, Computer",
            "Tomography, X-Ray Computed"
        ],
        "No. of Cites": "2275",
        "Views": 21285
    },
    {
        "Title": "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM",
        "Authors": [
            "Carlos Campos",
            "Richard Elvira",
            "Juan J. Gómez Rodríguez",
            "José M. M. Montiel",
            "Juan D. Tardós"
        ],
        "Abstract": "This article presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multimap SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a tightly integrated visual-inertial SLAM system that fully relies on maximum <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">a posteriori</i> (MAP) estimation, even during IMU initialization, resulting in real-time robust operation in small and large, indoor and outdoor environments, being two to ten times more accurate than previous approaches. The second main novelty is a multiple map system relying on a new place recognition method with improved recall that lets ORB-SLAM3 survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting them. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information from high parallax co-visible keyframes, even if they are widely separated in time or come from previous mapping sessions, boosting accuracy. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.5 cm in the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, representative of AR/VR scenarios. For the benefit of the community we make public the source code.",
        "Keywords": [
            "Simultaneous localization and mapping",
            "Computer vision",
            "Feature extraction",
            "Inertial navigation",
            "Robustness",
            "Optimization",
            "Simultaneous Localization And Mapping",
            "Visual System",
            "Indoor Environments",
            "Inertial Measurement Unit",
            "Depth Camera",
            "Previous Mapping",
            "Mapping System",
            "Maximum A Posteriori",
            "Stereo Camera",
            "Fisheye Lens",
            "Sensor Configuration",
            "Visual Odometry",
            "Place Recognition",
            "Activation Maps",
            "Posterior Mode",
            "Map Points",
            "Vision Sensors",
            "Camera Model",
            "Camera Pose",
            "Bundle Adjustment",
            "Loop Closure",
            "Pinhole Camera",
            "Reprojection Error",
            "Pinhole Camera Model",
            "Visual-inertial Odometry",
            "Body Velocity",
            "Inertial Parameters",
            "Scale Error",
            "Short-term Data",
            "Computer vision",
            "inertial navigation",
            "simult- aneous localization and mapping"
        ],
        "No. of Cites": "1993",
        "Views": 52964
    },
    {
        "Title": "Event-Based Vision: A Survey",
        "Authors": [
            "Guillermo Gallego",
            "Tobi Delbrück",
            "Garrick Orchard",
            "Chiara Bartolozzi",
            "Brian Taba",
            "Andrea Censi",
            "Stefan Leutenegger",
            "Andrew J. Davison",
            "Jörg Conradt",
            "Kostas Daniilidis",
            "Davide Scaramuzza"
        ],
        "Abstract": "Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of <inline-formula><tex-math notation=\"LaTeX\">$\\mu$</tex-math></inline-formula>s), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",
        "Keywords": [
            "Cameras",
            "Voltage control",
            "Brightness",
            "Robot vision systems",
            "Retina",
            "Event-based Vision",
            "High-resolution",
            "Computer Vision",
            "Power Consumption",
            "Feature Detection",
            "Optical Flow",
            "Spiking Neural Networks",
            "High Dynamic Range",
            "Feature Tracking",
            "Motion Blur",
            "Brightness Changes",
            "Conventional Camera",
            "Event Stream",
            "Low-level Vision",
            "Dynamic Vision Sensor",
            "Artificial Neural Network",
            "Image Reconstruction",
            "Image Plane",
            "Direction Of Motion",
            "Low Latency",
            "Simultaneous Localization And Mapping",
            "Depth Estimation",
            "Motion Compensation",
            "Optical Flow Estimation",
            "Event Frames",
            "Camera Motion",
            "Natural Scenes",
            "Voxel Grid",
            "Standard Camera",
            "Iterative Closest Point",
            "Event cameras",
            "bio-inspired vision",
            "asynchronous sensor",
            "low latency",
            "high dynamic range",
            "low power",
            "Algorithms",
            "Neural Networks, Computer",
            "Robotics"
        ],
        "No. of Cites": "1014",
        "Views": 46588
    },
    {
        "Title": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "Authors": [
            "Lvmin Zhang",
            "Anyi Rao",
            "Maneesh Agrawala"
        ],
        "Abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",
        "Keywords": [
            "Training",
            "Image segmentation",
            "Computer vision",
            "Image coding",
            "Image edge detection",
            "Neural networks",
            "Computer architecture",
            "Diffusion Model",
            "Neural Network",
            "Convolutional Layers",
            "Multiple Conditions",
            "Neural Architecture",
            "Spatial Control",
            "Human Pose",
            "Semantic",
            "Diffusion Process",
            "Generative Adversarial Networks",
            "Latent Space",
            "Image Generation",
            "High-quality Images",
            "Complex Shapes",
            "Segmentation Map",
            "Average Rank",
            "Neural Layers",
            "Inpainting",
            "Image Editing",
            "Training Domain",
            "Catastrophic Forgetting",
            "Fréchet Inception Distance",
            "Vision Transformer",
            "Latent Image",
            "StyleGAN",
            "Gaussian Weighting",
            "Concurrent Work",
            "User Study",
            "Computer Vision",
            "Feature Maps"
        ],
        "No. of Cites": "960",
        "Views": 1269
    },
    {
        "Title": "Vision Transformers for Dense Prediction",
        "Authors": [
            "René Ranftl",
            "Alexey Bochkovskiy",
            "Vladlen Koltun"
        ],
        "Abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
        "Keywords": [
            "Computer vision",
            "Image resolution",
            "Semantics",
            "Neural networks",
            "Estimation",
            "Training data",
            "Computer architecture",
            "Vision Transformer",
            "Dense Prediction",
            "State Of The Art",
            "Small Datasets",
            "Receptive Field",
            "Semantic Segmentation",
            "Global Field",
            "Fully Convolutional Network",
            "Monocular Depth Estimation",
            "Training Set",
            "Validation Set",
            "Convolutional Layers",
            "Input Image",
            "Image Classification",
            "Feature Maps",
            "Feature Dimension",
            "Visual Comparison",
            "Supplementary Materials For Details",
            "Transformer Model",
            "Stages Of Transformation",
            "Embedding Procedure",
            "Transformer Encoder",
            "Transformer Layers",
            "Input Resolution",
            "Multi-head Self-attention",
            "Image Embedding",
            "Network Embedding",
            "Input Patch",
            "Backbone Architecture",
            "Divisible",
            "Machine learning architectures and formulations",
            "3D from a single image and shape-from-x",
            "Segmentation",
            "grouping and shape"
        ],
        "No. of Cites": "949",
        "Views": 1552
    },
    {
        "Title": "Deep Learning for Image Super-Resolution: A Survey",
        "Authors": [
            "Zhihao Wang",
            "Jian Chen",
            "Steven C. H. Hoi"
        ],
        "Abstract": "Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",
        "Keywords": [
            "Deep learning",
            "Degradation",
            "Animals",
            "Benchmark testing",
            "Measurement",
            "Deep Learning",
            "Image Resolution",
            "Convolutional Neural Network",
            "High-resolution Images",
            "Feature Maps",
            "Learning Strategies",
            "Receptive Field",
            "Deep Convolutional Neural Network",
            "Perception Of Quality",
            "Peak Signal-to-noise Ratio",
            "Dense Connections",
            "Low-resolution Images",
            "Bilinear Interpolation",
            "Residual Learning",
            "Real-world Images",
            "Bicubic Interpolation",
            "Neural Architecture Search",
            "Super-resolution Network",
            "Mean Opinion Score",
            "Transposed Convolution Layers",
            "Super-resolution Model",
            "Pixel Loss",
            "Scaling Factor",
            "Depth Map",
            "Motion Compensation",
            "Image Quality",
            "Dense Block",
            "Input Image",
            "Low-dimensional Space",
            "Super-resolution Approaches",
            "Image super-resolution",
            "deep learning",
            "convolutional neural networks (CNN)",
            "Generative adversarial nets (GAN)"
        ],
        "No. of Cites": "948",
        "Views": 29642
    },
    {
        "Title": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding",
        "Authors": [
            "Jun Liu",
            "Amir Shahroudy",
            "Mauricio Perez",
            "Gang Wang",
            "Ling-Yu Duan",
            "Alex C. Kot"
        ],
        "Abstract": "Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding.",
        "Keywords": [
            "Three-dimensional displays",
            "Benchmark testing",
            "Cameras",
            "Deep learning",
            "Semantics",
            "Lighting",
            "Skeleton",
            "Human Activities",
            "Deep Learning",
            "Large-scale Datasets",
            "Action Recognition",
            "Action Classes",
            "Camera View",
            "Human Activity Recognition",
            "Video Samples",
            "Convolutional Neural Network",
            "Body Parts",
            "Deep Models",
            "Data Modalities",
            "Depth Map",
            "Recognition Performance",
            "Depth Data",
            "Frontal View",
            "3D Information",
            "Relevance Score",
            "Hand Motion",
            "Action Pair",
            "RGB Data",
            "Skeleton Data",
            "Auxiliary Set",
            "RGB Video",
            "Evaluation Of Classes",
            "Mutual Activation",
            "Recognition Framework",
            "3D Joint",
            "Recurrent Neural Network",
            "Finger Motion",
            "Activity understanding",
            "video analysis",
            "3D action recognition",
            "RGB+D vision",
            "deep learning",
            "large-scale benchmark",
            "Algorithms",
            "Benchmarking",
            "Deep Learning",
            "Human Activities",
            "Humans",
            "Image Processing, Computer-Assisted",
            "Pattern Recognition, Automated",
            "Semantics",
            "Video Recording"
        ],
        "No. of Cites": "918",
        "Views": 4793
    },
    {
        "Title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer",
        "Authors": [
            "René Ranftl",
            "Katrin Lasinger",
            "David Hafner",
            "Konrad Schindler",
            "Vladlen Koltun"
        ],
        "Abstract": "The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with five diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">zero-shot cross-dataset transfer</i>, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.",
        "Keywords": [
            "Training",
            "Estimation",
            "Three-dimensional displays",
            "Cameras",
            "Videos",
            "Measurement",
            "Motion pictures",
            "Depth Estimation",
            "Mixed Dataset",
            "Monocular Depth Estimation",
            "Zero-shot Cross-dataset Transfer",
            "Training Set",
            "Training Dataset",
            "State Of The Art",
            "Multiple Datasets",
            "3D Video",
            "Complementary Sources",
            "Ground Truth Depth",
            "Loss Function",
            "Environmental Variables",
            "Aspect Ratio",
            "Input Image",
            "Diverse Data",
            "ImageNet",
            "Indoor Environments",
            "Individual Datasets",
            "Unknown Scale",
            "Stereo Camera",
            "Static Scenes",
            "Stereo Pairs",
            "Disparity Range",
            "Dynamic Scenes",
            "Relative Depth",
            "Disparity Map",
            "Best-performing Model",
            "Stereo Images",
            "Monocular depth estimation",
            "single-image depth prediction",
            "zero-shot cross-dataset transfer",
            "multi-dataset training",
            "Algorithms"
        ],
        "No. of Cites": "755",
        "Views": 16635
    },
    {
        "Title": "LoFTR: Detector-Free Local Feature Matching with Transformers",
        "Authors": [
            "Jiaming Sun",
            "Zehong Shen",
            "Yuang Wang",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "Abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/.",
        "Keywords": [
            "Location awareness",
            "Visualization",
            "Costs",
            "Feature detection",
            "Pose estimation",
            "Transforms",
            "Detectors",
            "Local Features",
            "Feature Matching",
            "Local Feature Matching",
            "Descriptive Characteristics",
            "Receptive Field",
            "Large Margin",
            "Feature Detection",
            "Final Level",
            "Global Field",
            "Attention Layer",
            "Visual Localization",
            "Transformer Layers",
            "Cost Volume",
            "Convolutional Neural Network",
            "Image Pairs",
            "Depth Map",
            "Evaluation Protocol",
            "Repetitive Patterns",
            "Cyanoacrylate",
            "Pose Estimation",
            "Simultaneous Localization And Mapping",
            "Positional Encoding",
            "Mutual Nearest Neighbors",
            "Graph Neural Networks",
            "Camera Pose",
            "Local Feature Extraction",
            "Optimal Transport",
            "Structure From Motion",
            "Nearest Neighbor Search",
            "Outdoor Scenes"
        ],
        "No. of Cites": "700",
        "Views": 2015
    },
    {
        "Title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
        "Authors": [
            "Albert Pumarola",
            "Enric Corona",
            "Gerard Pons-Moll",
            "Francesc Moreno-Noguer"
        ],
        "Abstract": "Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be available at [1].",
        "Keywords": [
            "Photorealism",
            "Computer vision",
            "Three-dimensional displays",
            "Dynamics",
            "Machine learning",
            "Cameras",
            "Rendering (computer graphics)",
            "Dynamic Scenes",
            "Radiance Field",
            "Neural Radiance Fields",
            "Volume Density",
            "Camera View",
            "Single Camera",
            "Fully-connected Network",
            "View Direction",
            "Geometric Reasoning",
            "View Of The Scene",
            "Non-rigid Motion",
            "Neural Network",
            "Multilayer Perceptron",
            "RGB Images",
            "Time Instants",
            "Peak Signal-to-noise Ratio",
            "Optical Flow",
            "Scene Images",
            "3D Point",
            "Light Field",
            "Canonical Networks",
            "View Synthesis",
            "Volume Rendering",
            "None Of These Methods",
            "Camera Pose",
            "Ray Casting",
            "Emission Color",
            "Monocular Camera",
            "Single Viewpoint",
            "Depth Estimation"
        ],
        "No. of Cites": "605",
        "Views": 944
    },
    {
        "Title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
        "Authors": [
            "Jonathan T. Barron",
            "Ben Mildenhall",
            "Dor Verbin",
            "Pratul P. Srinivasan",
            "Peter Hedman"
        ],
        "Abstract": "Though neural radiance fields (NeRF) have demon-strated impressive view synthesis results on objects and small bounded regions of space, they struggle on “un-bounded” scenes, where the camera may point in any di-rection and content may exist at any distance. In this set-ting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the chal-lenges presented by unbounded scenes. Our model, which we dub “mip-NeRF 360” as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.",
        "Keywords": [
            "Computer vision",
            "Three-dimensional displays",
            "Nonlinear distortion",
            "Machine learning",
            "Cameras",
            "Rendering (computer graphics)",
            "Sensors",
            "Neural Radiance Fields",
            "Mean Square Error",
            "Multilayer Perceptron",
            "Euclidean Space",
            "3D Coordinates",
            "Volumetric Density",
            "Scene Content",
            "View Synthesis",
            "Histogram",
            "Training Time",
            "Input Image",
            "Weight Vector",
            "Supplementary Video",
            "Visual Perspective",
            "Inverse Distance",
            "Reconstruction Loss",
            "Projective Space",
            "Gradient Loss",
            "Positional Encoding",
            "Resampling Strategy",
            "Scene Geometry",
            "3D from multi-view and sensors; Machine learning"
        ],
        "No. of Cites": "581",
        "Views": 792
    },
    {
        "Title": "Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision",
        "Authors": [
            "Michael Niemeyer",
            "Lars Mescheder",
            "Michael Oechsle",
            "Andreas Geiger"
        ],
        "Abstract": "Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",
        "Keywords": [
            "Three-dimensional displays",
            "Rendering (computer graphics)",
            "Shape",
            "Image reconstruction",
            "Two dimensional displays",
            "Geometry",
            "Training",
            "3D Representation",
            "Implicit Learning",
            "Volume Rendering",
            "3D Supervision",
            "3D Reconstruction",
            "Learning-based Methods",
            "RGB Images",
            "Shape Representation",
            "Implicit Representation",
            "Depth Gradient",
            "Texture Representation",
            "Single Image",
            "Qualitative Results",
            "Network Parameters",
            "3D Space",
            "Depth Map",
            "3D Shape",
            "Learning-based Approaches",
            "Depth Values",
            "Forward Pass",
            "Surface Depth",
            "Backward Pass",
            "3D Texture",
            "Multi-view Images",
            "Multi-view Stereo",
            "Voxel Grid",
            "Small Resolution",
            "Automatic Differentiation",
            "Trimming Parameters",
            "Supervision Signal"
        ],
        "No. of Cites": "567",
        "Views": 559
    },
    {
        "Title": "Evolving Deep Convolutional Neural Networks for Image Classification",
        "Authors": [
            "Yanan Sun",
            "Bing Xue",
            "Mengjie Zhang",
            "Gary G. Yen"
        ],
        "Abstract": "Evolutionary paradigms have been successfully applied to neural network designs for two decades. Unfortunately, these methods cannot scale well to the modern deep neural networks due to the complicated architectures and large quantities of connection weights. In this paper, we propose a new method using genetic algorithms for evolving the architectures and connection weight initialization values of a deep convolutional neural network to address image classification problems. In the proposed algorithm, an efficient variable-length gene encoding strategy is designed to represent the different building blocks and the potentially optimal depth in convolutional neural networks. In addition, a new representation scheme is developed for effectively initializing connection weights of deep convolutional neural networks, which is expected to avoid networks getting stuck into local minimum that is typically a major issue in the backward gradient-based optimization. Furthermore, a novel fitness evaluation method is proposed to speed up the heuristic search with substantially less computational resource. The proposed algorithm is examined and compared with 22 existing algorithms on nine widely used image classification tasks, including the state-of-the-art methods. The experimental results demonstrate the remarkable superiority of the proposed algorithm over the state-of-the-art designs in terms of classification error rate and the number of parameters (weights).",
        "Keywords": [
            "Computer architecture",
            "Architecture",
            "Optimization",
            "Genetic algorithms",
            "Encoding",
            "Task analysis",
            "Convolutional neural networks",
            "Neural Network",
            "Convolutional Neural Network",
            "Deep Neural Network",
            "Image Classification",
            "Deep Convolutional Neural Network",
            "Building Blocks",
            "Computational Resources",
            "Classification Error",
            "Connection Weights",
            "Gradient-based Optimization",
            "Image Classification Tasks",
            "Weight Initialization",
            "Encoding Genes",
            "Classification Error Rate",
            "Encoding Strategies",
            "Deep Learning",
            "Classification Accuracy",
            "Classification Performance",
            "Convolutional Layers",
            "Feature Maps",
            "Crossover Operator",
            "Number Of Weights",
            "Standard Derivation",
            "Benchmark Datasets",
            "Deep Belief Network",
            "Promising Performance",
            "Pooling Layer",
            "Xavier Initialization",
            "Architectural Design",
            "Convolution Operation",
            "Convolutional neural network (CNN)",
            "deep learning",
            "genetic algorithms (GAs)",
            "image classification"
        ],
        "No. of Cites": "485",
        "Views": 8295
    },
    {
        "Title": "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching",
        "Authors": [
            "Xiaodong Gu",
            "Zhiwen Fan",
            "Siyu Zhu",
            "Zuozhuo Dai",
            "Feitong Tan",
            "Ping Tan"
        ],
        "Abstract": "The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the output depth or disparity. These methods are limited when high-resolution outputs are needed since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a both memory and time efficient cost volume formulation that is complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a standard feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the depth (or disparity) map from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner. We apply the cascade cost volume to the representative MVS-Net, and obtain a 35.6% improvement on DTU benchmark (1st place), with 50.6% and 59.3% reduction in GPU memory and run-time. It is also the state-of-the-art learning-based method on Tanks and Temples benchmark. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method. Our source code is available at https://github.com/alibaba/cascade-stereo.",
        "Keywords": [
            "Three-dimensional displays",
            "Graphics processing units",
            "Two dimensional displays",
            "Feature extraction",
            "Standards",
            "Cameras",
            "Spatial resolution",
            "Stereo Matching",
            "Multi-view Stereo",
            "Cost Volume",
            "Cascade Cost Volume",
            "Convolutional Neural Network",
            "Previous Stage",
            "3D Volume",
            "Depth Range",
            "Feature Pyramid",
            "GPU Memory",
            "Depth Interval",
            "Adaptive Adjustment",
            "Disparity Range",
            "Spatial Resolution",
            "Feature Maps",
            "Qualitative Results",
            "Large-scale Datasets",
            "Point Cloud",
            "Learning-based Methods",
            "Depth Map",
            "Feature Volume",
            "Number Of Planes",
            "Disparity Estimation",
            "Matching Cost",
            "Feature Pyramid Network",
            "Disparity Map",
            "Semantic Features",
            "Learning-based Approaches",
            "Original Image Size",
            "Cost Metrics"
        ],
        "No. of Cites": "455",
        "Views": 1265
    },
    {
        "Title": "Contrastive Learning for Compact Single Image Dehazing",
        "Authors": [
            "Haiyan Wu",
            "Yanyun Qu",
            "Shaohui Lin",
            "Jian Zhou",
            "Ruizhi Qiao",
            "Zhizhong Zhang",
            "Yuan Xie",
            "Lizhuang Ma"
        ],
        "Abstract": "Single image dehazing is a challenging ill-posed problem due to the severe information degeneration. However, existing deep learning based dehazing methods only adopt clear images as positive samples to guide the training of dehazing network while negative information is unexploited. Moreover, most of them focus on strengthening the dehazing network with an increase of depth and width, leading to a significant requirement of computation and memory. In this paper, we propose a novel contrastive regularization (CR) built upon contrastive learning to exploit both the information of hazy images and clear images as negative and positive samples, respectively. CR ensures that the restored image is pulled to closer to the clear image and pushed to far away from the hazy image in the representation space.Furthermore, considering trade-off between performance and memory storage, we develop a compact dehazing network based on autoencoder-like (AE) framework. It involves an adaptive mixup operation and a dynamic feature enhancement module, which can benefit from preserving information flow adaptively and expanding the receptive field to improve the network’s transformation capability, respectively. We term our dehazing network with autoencoder and contrastive regularization as AECR-Net. The extensive experiments on synthetic and real-world datasets demonstrate that our AECR-Net surpass the state-of-the-art approaches. The code is released in https://github.com/GlassyWu/AECR-Net.",
        "Keywords": [
            "Training",
            "Deep learning",
            "Computer vision",
            "Adaptive systems",
            "Codes",
            "Memory management",
            "Performance gain",
            "Self-supervised Learning",
            "Image Dehazing",
            "Single Image Dehazing",
            "Positive Samples",
            "Negative Samples",
            "Image Information",
            "Receptive Field",
            "Representation Of Space",
            "Real-world Datasets",
            "Image Space",
            "Dynamic Mode",
            "Clear Image",
            "Lower Bound",
            "Deep Neural Network",
            "Convolutional Layers",
            "Latent Space",
            "Regularization Term",
            "Skip Connections",
            "Reduction In Parameters",
            "Shallow Layers",
            "State Of The Art Methods",
            "Shallow Features",
            "Downsampling Layer",
            "Deformable Convolution",
            "Color Distortion",
            "Transmission Map",
            "Atmospheric Light",
            "Reconstruction Loss",
            "Unsatisfying Results",
            "Contrastive Loss"
        ],
        "No. of Cites": "446",
        "Views": 1475
    },
    {
        "Title": "3D Packing for Self-Supervised Monocular Depth Estimation",
        "Authors": [
            "Vitor Guizilini",
            "Rareș Ambruș",
            "Sudeep Pillai",
            "Allan Raventos",
            "Adrien Gaidon"
        ],
        "Abstract": "Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.",
        "Keywords": [
            "Estimation",
            "Training",
            "Three-dimensional displays",
            "Image resolution",
            "Cameras",
            "Laser radar",
            "Task analysis",
            "Depth Estimation",
            "3D Software",
            "Monocular Depth Estimation",
            "Self-supervised Monocular Depth Estimation",
            "Self-driving",
            "New Urban",
            "3D Convolution",
            "Automated Vehicles",
            "Input Resolution",
            "Accurate Depth",
            "Ground Truth Depth",
            "Dense Depth",
            "High-resolution",
            "Convolutional Layers",
            "Spatial Information",
            "Training Time",
            "Single Image",
            "Target Image",
            "Feature Channels",
            "Depth Map",
            "Nearest Neighbor Interpolation",
            "Unlabeled Data",
            "ResNet Architecture",
            "3D Convolutional Layers",
            "Network Depth",
            "Instantaneous Velocity",
            "Scale Ambiguity",
            "Source Images",
            "Scene Depth",
            "Inherent Ambiguity"
        ],
        "No. of Cites": "416",
        "Views": 721
    },
    {
        "Title": "Real-World Underwater Enhancement: Challenges, Benchmarks, and Solutions Under Natural Light",
        "Authors": [
            "Risheng Liu",
            "Xin Fan",
            "Ming Zhu",
            "Minjun Hou",
            "Zhongxuan Luo"
        ],
        "Abstract": "Underwater image enhancement is such an important low-level vision task with many applications that numerous algorithms have been proposed in recent years. These algorithms developed upon various assumptions demonstrate successes from various aspects using different data sets and different metrics. In this work, we setup an undersea image capturing system, and construct a large-scale Real-world Underwater Image Enhancement (RUIE) data set divided into three subsets. The three subsets target at three challenging aspects for enhancement, i.e., image visibility quality, color casts, and higher-level detection/classification, respectively. We conduct extensive and systematic experiments on RUIE to evaluate the effectiveness and limitations of various algorithms to enhance visibility and correct color casts on images with hierarchical categories of degradation. Moreover, underwater image enhancement in practice usually serves as a preprocessing step for mid-level and high-level vision tasks. We thus exploit the object detection performance on enhanced images as a brand new task-specific evaluation criterion. The findings from these evaluations not only confirm what is commonly believed, but also suggest promising solutions and new directions for visibility enhancement, color correction, and object detection on real-world underwater images. The benchmark is available at: https://github.com/dlut-dimt/Realworld-Underwater-Image-Enhancement-RUIE-Benchmark.",
        "Keywords": [
            "Image color analysis",
            "Image enhancement",
            "Task analysis",
            "Histograms",
            "Benchmark testing",
            "Imaging",
            "Degradation",
            "Daylight",
            "Underwater Enhancement",
            "Object Detection",
            "Vision Tasks",
            "Numerical Algorithm",
            "Image Enhancement",
            "Undersea",
            "Real-world Images",
            "Color Correction",
            "Visual Enhancement",
            "Underwater Image",
            "Convolutional Neural Network",
            "Image Quality",
            "Computer Vision",
            "Image Contrast",
            "Generative Adversarial Networks",
            "Saline Water",
            "Training Examples",
            "Sea Urchin",
            "Light Attenuation",
            "Enhancement Algorithm",
            "Autonomous Underwater Vehicles",
            "Model-free Methods",
            "Transmission Map",
            "Sea Cucumber",
            "Scene Depth",
            "Comprehensive Metrics",
            "White Balance",
            "Histogram Equalization",
            "Underwater image enhancement",
            "benchmark",
            "visibility",
            "color cast",
            "object detection"
        ],
        "No. of Cites": "409",
        "Views": 5653
    },
    {
        "Title": "Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks",
        "Authors": [
            "Deng-Ping Fan",
            "Zheng Lin",
            "Zhao Zhang",
            "Menglong Zhu",
            "Ming-Ming Cheng"
        ],
        "Abstract": "The use of RGB-D information for salient object detection (SOD) has been extensively explored in recent years. However, relatively few efforts have been put toward modeling SOD in real-world human activity scenes with RGB-D. In this article, we fill the gap by making the following contributions to RGB-D SOD: 1) we carefully collect a new <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">S</b>al<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</b>ent <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">P</b>erson (SIP) data set that consists of ~1 K high-resolution images that cover diverse real-world scenes from various viewpoints, poses, occlusions, illuminations, and background s; 2) we conduct a large-scale (and, so far, the most comprehensive) benchmark comparing contemporary methods, which has long been missing in the field and can serve as a baseline for future research, and we systematically summarize 32 popular models and evaluate 18 parts of 32 models on seven data sets containing a total of about 97k images; and 3) we propose a simple general architecture, called deep depth-depurator network (D<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>Net). It consists of a depth depurator unit (DDU) and a three-stream feature learning module (FLM), which performs low-quality depth map filtering and cross-modal feature learning, respectively. These components form a nested structure and are elaborately designed to be learned jointly. D<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>Net exceeds the performance of any prior contenders across all five metrics under consideration, thus serving as a strong model to advance research in this field. We also demonstrate that D<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>Net can be used to efficiently extract salient object masks from real scenes, enabling effective background-changing application with a speed of 65 frames/s on a single GPU. All the saliency maps, our new SIP data set, the D<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3</sup>Net model, and the evaluation tools are publicly available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/DengPingFan/D3NetBenchmark</uri>.",
        "Keywords": [
            "Data models",
            "Benchmark testing",
            "Measurement",
            "Smart phones",
            "Cameras",
            "Lighting",
            "Learning systems",
            "Benchmark",
            "Salient Object",
            "Salient Object Detection",
            "RGB-D Salient Object Detection",
            "Depth Map",
            "Real Scenes",
            "Saliency Map",
            "Real-world Scenes",
            "Mobile Phone",
            "Deep Models",
            "Training Phase",
            "Test Phase",
            "RGB Images",
            "Changes In Appearance",
            "Light Field",
            "Depth Camera",
            "Challenging Situations",
            "Challenging Dataset",
            "RGB-D Dataset",
            "Center Bias",
            "RGB-D Images",
            "Saliency Detection",
            "Background Scene",
            "Depth Features",
            "Smartphone Camera",
            "Benchmark",
            "RGB-D",
            "saliency",
            "salient object detection (SOD)",
            "Salient Person (SIP) data set"
        ],
        "No. of Cites": "400",
        "Views": 3270
    },
    {
        "Title": "Residual Feature Aggregation Network for Image Super-Resolution",
        "Authors": [
            "Jie Liu",
            "Wenjie Zhang",
            "Yuting Tang",
            "Jie Tang",
            "Gangshan Wu"
        ],
        "Abstract": "Recently, very deep convolutional neural networks (CNNs) have shown great power in single image super-resolution (SISR) and achieved significant improvements against traditional methods. Among these CNN-based methods, the residual connections play a critical role in boosting the network performance. As the network depth grows, the residual features gradually focused on different aspects of the input image, which is very useful for reconstructing the spatial details. However, existing methods neglect to fully utilize the hierarchical features on the residual branches. To address this issue, we propose a novel residual feature aggregation (RFA) framework for more efficient feature extraction. The RFA framework groups several residual modules together and directly forwards the features on each local residual branch by adding skip connections. Therefore, the RFA framework is capable of aggregating these informative residual features to produce more representative features. To maximize the power of the RFA framework, we further propose an enhanced spatial attention (ESA) block to make the residual features to be more focused on critical spatial contents. The ESA block is designed to be lightweight and efficient. Our final RFANet is constructed by applying the proposed RFA framework with the ESA blocks. Comprehensive experiments demonstrate the necessity of our RFA framework and the superiority of our RFANet over state-of-the-art SISR methods.",
        "Keywords": [
            "Feature extraction",
            "Convolution",
            "Spatial resolution",
            "Image reconstruction",
            "Training",
            "Task analysis",
            "Super-resolution",
            "Residual Network",
            "Residual Feature",
            "Residual Aggregation",
            "Convolutional Neural Network",
            "Deep Network",
            "Deep Neural Network",
            "Feature Representation",
            "Deep Convolutional Neural Network",
            "Spatial Attention",
            "Skip Connections",
            "CNN-based Methods",
            "Hierarchical Features",
            "Aspects Of Image",
            "Residual Module",
            "Spatial Content",
            "Single Image Super-resolution",
            "Convolutional Layers",
            "Feature Maps",
            "Attention Mechanism",
            "Residual Block",
            "Attention Block",
            "Dense Block",
            "End Of Module",
            "Low-resolution Images",
            "Spatial Attention Mechanism",
            "Memory Block",
            "Super-resolution Network",
            "Residual Learning",
            "Part Of The Trunk"
        ],
        "No. of Cites": "391",
        "Views": 1641
    },
    {
        "Title": "Neural Style Transfer: A Review",
        "Authors": [
            "Yongcheng Jing",
            "Yezhou Yang",
            "Zunlei Feng",
            "Jingwen Ye",
            "Yizhou Yu",
            "Mingli Song"
        ],
        "Abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://osf.io/f8tu4/</uri>.",
        "Keywords": [
            "Rendering (computer graphics)",
            "Painting",
            "Taxonomy",
            "Visualization",
            "Convolutional neural networks",
            "Art",
            "Shape",
            "Style Transfer",
            "Neural Style Transfer",
            "Convolutional Neural Network",
            "Information Content",
            "Feature Maps",
            "Image Reconstruction",
            "Feed-forward Network",
            "Depth Information",
            "Loss Of Content",
            "Pre-trained Network",
            "Markov Random Field",
            "Squared Euclidean Distance",
            "Variety Of Styles",
            "Convolutional Neural Network Features",
            "Gram Matrix",
            "Style Features",
            "Adversarial Examples",
            "Style Image",
            "Instance Normalization",
            "Multiple Styles",
            "Texture Model",
            "Brush Strokes",
            "Disentangled Representation",
            "Filter Response",
            "Explicit Restrictions",
            "Image Regions",
            "Types Of Images",
            "Optical Flow",
            "Series Of Transformations",
            "Content Features",
            "Neural style transfer (NST)",
            "convolutional neural network (CNN)"
        ],
        "No. of Cites": "378",
        "Views": 17777
    },
    {
        "Title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free",
        "Authors": [
            "Kangle Deng",
            "Andrew Liu",
            "Jun-Yan Zhu",
            "Deva Ramanan"
        ],
        "Abstract": "A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as “free” depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.",
        "Keywords": [
            "Training",
            "Geometry",
            "Three-dimensional displays",
            "Uncertainty",
            "Pipelines",
            "Rendering (computer graphics)",
            "Surface fitting",
            "Neural Radiance Fields",
            "Fewer Views",
            "Depth Camera",
            "Camera Pose",
            "Sparse Point",
            "Scene Geometry",
            "Volume Rendering",
            "Point Cloud",
            "Kullback-Leibler",
            "Continuous Distribution",
            "Depth Map",
            "Volume Density",
            "Redwood",
            "Training Iterations",
            "Identical Distribution",
            "Training Speed",
            "Depth Measurements",
            "Implicit Function",
            "Multiple Viewpoints",
            "Reprojection Error",
            "Sparse Point Cloud",
            "View Synthesis",
            "Depth Error",
            "Dense Depth",
            "3D from multi-view and sensors; RGBD sensors and analytics; Vision + graphics"
        ],
        "No. of Cites": "375",
        "Views": 728
    },
    {
        "Title": "NICE-SLAM: Neural Implicit Scalable Encoding for SLAM",
        "Authors": [
            "Zihan Zhu",
            "Songyou Peng",
            "Viktor Larsson",
            "Weiwei Xu",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Martin R. Oswald",
            "Marc Pollefeys"
        ],
        "Abstract": "Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over- smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam.",
        "Keywords": [
            "Geometry",
            "Visualization",
            "Computer vision",
            "Simultaneous localization and mapping",
            "Scalability",
            "Network architecture",
            "Cameras",
            "Scalable",
            "Neural Representations",
            "Simultaneous Localization And Mapping",
            "Density Of The System",
            "Hierarchical Representation",
            "Scene Representation",
            "Multilayer Perceptron",
            "Final Level",
            "Depth Map",
            "Depth Images",
            "Current Frame",
            "Hierarchical Architecture",
            "Floating-point Operations",
            "Tracking Results",
            "Partial Observation",
            "Camera Pose",
            "Local Updates",
            "Real-time Capability",
            "Interesting Future Direction",
            "Voxel Grid",
            "Grid Features",
            "Scene Geometry",
            "Geometric Loss",
            "Occupancy Values",
            "Color Representation",
            "Bundle Adjustment",
            "Replica",
            "Sample Pixels",
            "Dynamic Objects",
            "L1 Loss",
            "3D from multi-view and sensors; RGBD sensors and analytics"
        ],
        "No. of Cites": "350",
        "Views": 1412
    },
    {
        "Title": "RIFT: Multi-Modal Image Matching Based on Radiation-Variation Insensitive Feature Transform",
        "Authors": [
            "Jiayuan Li",
            "Qingwu Hu",
            "Mingyao Ai"
        ],
        "Abstract": "Traditional feature matching methods, such as scale-invariant feature transform (SIFT), usually use image intensity or gradient information to detect and describe feature points; however, both intensity and gradient are sensitive to nonlinear radiation distortions (NRD). To solve this problem, this paper proposes a novel feature matching algorithm that is robust to large NRD. The proposed method is called radiation-variation insensitive feature transform (RIFT). There are three main contributions in RIFT. First, RIFT uses phase congruency (PC) instead of image intensity for feature point detection. RIFT considers both the number and repeatability of feature points and detects both corner points and edge points on the PC map. Second, RIFT originally proposes a maximum index map (MIM) for feature description. The MIM is constructed from the log-Gabor convolution sequence and is much more robust to NRD than traditional gradient map. Thus, RIFT not only largely improves the stability of feature detection but also overcomes the limitation of gradient information for feature description. Third, RIFT analyses the inherent influence of rotations on the values of the MIM and realises rotation invariance. We use six different types of multi-modal image datasets to evaluate RIFT, including optical-optical, infrared-optical, synthetic aperture radar (SAR)-optical, depth-optical, map-optical, and day-night datasets. Experimental results show that RIFT is superior to SIFT and SAR-SIFT on multi-modal images. To the best of our knowledge, RIFT is the first feature matching algorithm that can achieve good performance on all the abovementioned types of multi-modal images. The source code of RIFT and the multi-modal image datasets are publicly available.",
        "Keywords": [
            "Feature extraction",
            "Image matching",
            "Remote sensing",
            "Nonlinear distortion",
            "Optical distortion",
            "Detectors",
            "Image Registration",
            "Multimodal Imaging",
            "Multimodal Image Matching",
            "Radiation-variation Insensitive Feature Transform",
            "Descriptive Characteristics",
            "Image Intensity",
            "Synthetic Aperture Radar",
            "Feature Detection",
            "Feature Points",
            "Types Of Datasets",
            "Matching Algorithm",
            "Intensity Information",
            "Feature Matching",
            "Image Gradient",
            "Rotation Invariance",
            "Scale-invariant Feature Transform",
            "Corner Points",
            "Multimodal Dataset",
            "Feature Point Detection",
            "Root Mean Square Error",
            "Synthetic Aperture Radar Images",
            "Rotation Changes",
            "Image Pairs",
            "Distortion Types",
            "Feature-based Methods",
            "Geometric Distortion",
            "Number Of Orientations",
            "Speeded Up Robust Features",
            "Edge Features",
            "Illumination Changes",
            "Multi-modal image matching",
            "nonlinear radiation distortions (NRD)",
            "feature matching",
            "maximum index map (MIM)",
            "phase congruency (PC)"
        ],
        "No. of Cites": "337",
        "Views": 5719
    },
    {
        "Title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection",
        "Authors": [
            "Tai Wang",
            "Xinge Zhu",
            "Jiangmiao Pang",
            "Dahua Lin"
        ],
        "Abstract": "Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes. Then the objects are distributed to different feature levels with consideration of their 2D scales and assigned only according to the projected 3D-center for the training procedure. Furthermore, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.",
        "Keywords": [
            "Convolutional codes",
            "Training",
            "Three-dimensional displays",
            "Costs",
            "Estimation",
            "Object detection",
            "Transforms",
            "Object Detection",
            "3D Object Detection",
            "Monocular 3D Object Detection",
            "Conventional 2D",
            "3D Properties",
            "3D Detection",
            "3D Target",
            "Feature Maps",
            "Intersection Over Union",
            "Point Cloud",
            "Bounding Box",
            "Fundamental Problem",
            "RGB Images",
            "Average Precision",
            "Classification Loss",
            "Depth Estimation",
            "Binary Cross Entropy",
            "Feature Pyramid Network",
            "Detection Head",
            "3D Bounding Box",
            "Ground-truth Box",
            "2D Object",
            "Regression Branch",
            "Anchor-based Methods",
            "Flipped Images",
            "2D Location",
            "Single Image",
            "Feature Extraction Backbone"
        ],
        "No. of Cites": "334",
        "Views": 476
    },
    {
        "Title": "Attentional Local Contrast Networks for Infrared Small Target Detection",
        "Authors": [
            "Yimian Dai",
            "Yiquan Wu",
            "Fei Zhou",
            "Kobus Barnard"
        ],
        "Abstract": "To mitigate the issue of minimal intrinsic features for pure data-driven methods, in this article, we propose a novel model-driven deep network for infrared small target detection, which combines discriminative networks and conventional model-driven methods to make use of both labeled data and the domain knowledge. By designing a feature map cyclic shift scheme, we modularize a conventional local contrast measure method as a depthwise parameterless nonlinear feature refinement layer in an end-to-end network, which encodes relatively long-range contextual interactions with clear physical interpretability. To highlight and preserve the small target features, we also exploit a bottom-up attentional modulation integrating the smaller scale subtle details of low-level features into high-level features of deeper layers. We conduct detailed ablation studies with varying network depths to empirically verify the effectiveness and efficiency of the design of each component in our network architecture. We also compare the performance of our network against other model-driven methods and deep networks on the open SIRST data set as well. The results suggest that our network yields a performance boost over its competitors.",
        "Keywords": [
            "Feature extraction",
            "Object detection",
            "Task analysis",
            "Modulation",
            "Data models",
            "Semantics",
            "Acceleration",
            "Small Target",
            "Local Contrast",
            "Deep Network",
            "Cognitive Domains",
            "Feature Maps",
            "Deeper Layers",
            "Local Measurements",
            "Open Data",
            "Long-range Interactions",
            "High-level Features",
            "Attention Module",
            "Low-level Features",
            "Data-driven Methods",
            "Network Depth",
            "Clear Interpretation",
            "Cyclic Shift",
            "Subtle Details",
            "Convolutional Network",
            "Infrared Imaging",
            "Feature Pyramid Network",
            "Dilation Rate",
            "Intersection Over Union",
            "Open Dataset",
            "High-level Semantics",
            "Local Attention",
            "High-level Feature Maps",
            "Global Attention",
            "Receptive Field",
            "Baseline Network",
            "Attention mechanism",
            "deep learning",
            "feature fusion",
            "infrared small target",
            "local contrast"
        ],
        "No. of Cites": "329",
        "Views": 5204
    }
]